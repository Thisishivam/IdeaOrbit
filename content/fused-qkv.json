{
    "title": "QKV Projection with Tensor Parallelism",
    "meta": {
        "date": "Sep 10, 2025",
        "readTime": "13 min read"
    },
    "tags": ["Attention", "QKV", "Tensor Parallelism"],
    "toc": [
        "Purpose",
        "Implementation",
        "Benefits",
        "GPT-4 Scale Configuration",
        "Integration with Rotary Embeddings",
        "GPT-4 Style Implementation"
    ],
    "content": "<h2 id=\"purpose\">Purpose</h2><p>Project input embeddings into Query, Key, and Value tensors for attention mechanisms with efficient tensor parallelism support.</p><h2 id=\"implementation\">Implementation</h2><div class=\"code-block\">class ProductionQKVProjection(nn.Module):<br>&nbsp;&nbsp;def __init__(self, hidden_dim: int, num_heads: int, head_dim: int):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;self.hidden_dim = hidden_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.num_heads = num_heads<br>&nbsp;&nbsp;&nbsp;&nbsp;self.head_dim = head_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.world_size = dist.get_world_size()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Calculate dimensions per device<br>&nbsp;&nbsp;&nbsp;&nbsp;self.num_heads_per_rank = num_heads // self.world_size<br>&nbsp;&nbsp;&nbsp;&nbsp;self.proj_dim_per_rank = self.num_heads_per_rank * head_dim * 3  # Q, K, V<br><br>&nbsp;&nbsp;&nbsp;&nbsp;self.qkv_proj = ColumnParallelLinear(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hidden_dim, self.proj_dim_per_rank, bias=False<br>&nbsp;&nbsp;&nbsp;&nbsp;)<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:<br>&nbsp;&nbsp;&nbsp;&nbsp;batch_size, seq_len, _ = x.shape<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Project to QKV<br>&nbsp;&nbsp;&nbsp;&nbsp;qkv = self.qkv_proj(x)  # [batch, seq_len, num_heads_per_rank * head_dim * 3]<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Reshape and split into Q, K, V<br>&nbsp;&nbsp;&nbsp;&nbsp;qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads_per_rank, self.head_dim)<br>&nbsp;&nbsp;&nbsp;&nbsp;qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, num_heads_per_rank, seq_len, head_dim]<br><br>&nbsp;&nbsp;&nbsp;&nbsp;q, k, v = qkv[0], qkv[1], qkv[2]<br>&nbsp;&nbsp;&nbsp;&nbsp;return q, k, v</div><h2 id=\"benefits\">Benefits</h2><ul><li>Combines projection layers for efficiency (single large matmul instead of three separate ones)</li><li>Applies rotary embeddings to queries and keys</li><li>Distributes computation across multiple GPUs with tensor parallelism</li><li>Optimized memory access patterns</li></ul><h2 id=\"gpt4-config\">GPT-4 Scale Configuration</h2><ul><li><strong>Hidden Dimension:</strong> 8,192</li><li><strong>Number of Heads:</strong> 64</li><li><strong>Head Dimension:</strong> 128 (8192 / 64)</li><li><strong>QKV Dimension per Head:</strong> 128 × 3 = 384</li><li><strong>Total QKV Parameters:</strong> 8,192 × (64 × 128 × 3) = 201,326,592 parameters</li></ul><h2 id=\"integration\">Integration with Rotary Embeddings</h2><p>The QKV projection works seamlessly with our fused rotary embeddings, applying positional encoding to queries and keys before the attention computation.</p><div class=\"code-block\"># Example usage with rotary embeddings<br>qkv_proj = ProductionQKVProjection(8192, 64, 128)<br>rotary_emb = FusedRotaryEmbedding(128)<br><br>x = torch.randn(4, 4096, 8192, device='cuda', dtype=torch.bfloat16)<br>q, k, v = qkv_proj(x)<br><br># Apply rotary embeddings to queries and keys<br>q = rotary_emb(q, seq_len=4096)<br>k = rotary_emb(k, seq_len=4096)</div><h2 id=\"gpt4-style-implementation\">GPT-4 Style Implementation</h2><div class=\"code-block\">class ProductionQKVProjection(nn.Module):<br>&nbsp;&nbsp;def __init__(self, hidden_dim: int, num_heads: int, rotary_dim: int):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;assert hidden_dim % num_heads == 0<br>&nbsp;&nbsp;&nbsp;&nbsp;self.hidden_dim = hidden_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.num_heads = num_heads<br>&nbsp;&nbsp;&nbsp;&nbsp;self.head_dim = hidden_dim // num_heads<br>&nbsp;&nbsp;&nbsp;&nbsp;self.rotary_dim = rotary_dim<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Fused QKV projection with tensor parallelism<br>&nbsp;&nbsp;&nbsp;&nbsp;self.qkv = ColumnParallelLinear(hidden_dim, hidden_dim * 3, bias=False)<br>&nbsp;&nbsp;&nbsp;&nbsp;self.rotary_emb = FusedRotaryEmbedding(rotary_dim)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# GPT-4 style initialization<br>&nbsp;&nbsp;&nbsp;&nbsp;with torch.no_grad():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Attention projection specific scaling<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.qkv.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(2))<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor, positions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:<br>&nbsp;&nbsp;&nbsp;&nbsp;B, S, D = x.shape<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Fused QKV projection<br>&nbsp;&nbsp;&nbsp;&nbsp;qkv = self.qkv(x)  # [B, S, 3*D]<br>&nbsp;&nbsp;&nbsp;&nbsp;q, k, v = qkv.chunk(3, dim=-1)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Reshape for attention<br>&nbsp;&nbsp;&nbsp;&nbsp;q = q.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)<br>&nbsp;&nbsp;&nbsp;&nbsp;k = k.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)<br>&nbsp;&nbsp;&nbsp;&nbsp;v = v.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# Apply fused rotary embeddings<br>&nbsp;&nbsp;&nbsp;&nbsp;q, k = self.rotary_emb(q, k, positions)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;return q, k, v</div><h4>Detailed Explanation of the GPT-4 Style Implementation</h4><ul><li><strong>Hidden and Head Dimensions:</strong> Ensures the total hidden dimension is divisible by the number of heads, defining the head dimension used in attention computation.</li><li><strong>Fused Projection:</strong> Combines Q, K, and V into a single projection layer, significantly improving computation efficiency by reducing memory access and operations.</li><li><strong>Rotary Embeddings:</strong> Integrates fused rotary embeddings to add positional information directly to queries and keys, improving attention's contextual understanding.</li><li><strong>Initialization:</strong> Uses GPT-4 recommended initialization with scaled normal distribution to stabilize gradients and learning during training.</li><li><strong>Data Flow:</strong> Projects the input, splits into Q, K, V, reshapes and transposes them for multi-head attention, and finally applies rotary embeddings for positional context.</li><li><strong>Parallelism:</strong> The use of tensor parallelism in the projection layer allows distributing computation across multiple GPUs, enabling training of very large models.</li></ul><p>This GPT-4 style implementation ensures that the QKV projection is both memory-efficient and computationally optimized while adhering to best practices in large-scale transformer architectures.</p>"
}
