{
    "title": "Paged KV Cache: Efficient Long-Context Inference",
    "meta": {
        "date": "Sep 13, 2025",
        "readTime": "14 min read"
    },
    "tags": ["KV Cache", "Memory Optimization", "Long Context", "Inference"],
    "content": "<h2 id=\"introduction\">Introduction to Paged KV Cache</h2><p>The Paged KV Cache is a revolutionary approach to handling long-context inference in transformer models. Traditional KV caches require contiguous memory allocation, which becomes problematic for sequences longer than 32K tokens. Our paged implementation solves this by breaking the cache into manageable pages, enabling efficient processing of extremely long sequences.</p><p>This implementation is crucial for applications like document analysis, long-form content generation, and conversational AI where context windows can extend to hundreds of thousands of tokens.</p><h2 id=\"problem\">The Problem with Traditional KV Caches</h2><p>Standard KV caches face several critical limitations:</p><ul><li><strong>Memory Fragmentation:</strong> Large contiguous blocks are hard to allocate</li><li><strong>Fixed Size Limits:</strong> Maximum sequence length is predetermined</li><li><strong>Inefficient Memory Usage:</strong> Wasted space for shorter sequences</li><li><strong>No Dynamic Growth:</strong> Cannot adapt to varying sequence lengths</li></ul><h2 id=\"solution\">Paged Solution Architecture</h2><p>Our paged KV cache implementation uses a virtual memory-like system:</p><div class=\"code-block\">class PagedKVCache:\n    def __init__(self, batch_size: int, num_heads: int, head_dim: int, \n                 page_size: int = 256, max_pages: int = 512, \n                 dtype: torch.dtype = torch.bfloat16, device: torch.device = 'cuda'):\n        self.batch_size = batch_size\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.page_size = page_size\n        self.max_pages = max_pages\n        self.dtype = dtype\n        self.device = device\n\n        # Pre-allocate pages\n        self.k_pages = torch.zeros((max_pages, batch_size, num_heads, page_size, head_dim), dtype=dtype, device=device)\n        self.v_pages = torch.zeros((max_pages, batch_size, num_heads, page_size, head_dim), dtype=dtype, device=device)\n        self.page_table = torch.full((batch_size, max_pages), -1, dtype=torch.long, device=device)\n        self.sequence_lengths = torch.zeros(batch_size, dtype=torch.long, device=device)\n\n    def update(self, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        batch_size, num_heads, new_tokens, head_dim = k.shape\n        k_out, v_out = [], []\n\n        for b in range(batch_size):\n            current_len = self.sequence_lengths[b].item()\n\n            # Calculate required and current pages\n            required_pages = (current_len + new_tokens + self.page_size - 1) // self.page_size\n            current_pages = (current_len + self.page_size - 1) // self.page_size\n\n            # Allocate new pages if needed\n            for _ in range(required_pages - current_pages):\n                free_page = torch.nonzero(self.page_table[b] == -1, as_tuple=True)[0]\n                if len(free_page) == 0:\n                    raise ValueError(\"Out of KV cache pages\")\n                page_idx = free_page[0]\n                self.page_table[b, page_idx] = current_len\n\n            # Copy tokens into pages\n            tokens_copied = 0\n            while tokens_copied < new_tokens:\n                page_idx = (current_len + tokens_copied) // self.page_size\n                page_offset = (current_len + tokens_copied) % self.page_size\n\n                tokens_to_copy = min(new_tokens - tokens_copied, self.page_size - page_offset)\n\n                self.k_pages[page_idx, b, :, page_offset:page_offset + tokens_to_copy] = k[b, :, tokens_copied:tokens_copied + tokens_to_copy]\n                self.v_pages[page_idx, b, :, page_offset:page_offset + tokens_to_copy] = v[b, :, tokens_copied:tokens_copied + tokens_to_copy]\n\n                tokens_copied += tokens_to_copy\n\n            # Gather full sequence from pages\n            total_len = current_len + new_tokens\n            k_seq = torch.zeros(num_heads, total_len, head_dim, dtype=self.dtype, device=self.device)\n            v_seq = torch.zeros(num_heads, total_len, head_dim, dtype=self.dtype, device=self.device)\n\n            for pos in range(total_len):\n                page_idx = pos // self.page_size\n                page_offset = pos % self.page_size\n                k_seq[:, pos] = self.k_pages[page_idx, b, :, page_offset]\n                v_seq[:, pos] = self.v_pages[page_idx, b, :, page_offset]\n\n            k_out.append(k_seq.unsqueeze(0))\n            v_out.append(v_seq.unsqueeze(0))\n\n            self.sequence_lengths[b] = total_len\n\n        return torch.cat(k_out, dim=0), torch.cat(v_out, dim=0)</div><h3 id=\"detailed-explanation\">Detailed Explanation of Code</h3><ol><li><strong>Initialization:</strong> Pages for <code>K</code> and <code>V</code> are pre-allocated as 5D tensors: (max_pages, batch_size, num_heads, page_size, head_dim). A <code>page_table</code> tracks which logical positions map to which pages.</li><li><strong>update():</strong> When new tokens arrive, it computes how many pages are needed. If insufficient, new pages are allocated from the free pool.</li><li><strong>Token Copying:</strong> New keys/values are placed into the correct page and offset. This avoids reallocating large contiguous blocks.</li><li><strong>Sequence Gathering:</strong> To return the usable <code>K</code> and <code>V</code>, the method reconstructs the full sequence by collecting slices from pages in order.</li><li><strong>Dynamic Growth:</strong> Each call increases <code>sequence_lengths</code> for the batch, allowing variable sequence lengths per sample.</li></ol><h2 id=\"demo\">Beginner-Friendly Demo</h2><p>Hereâ€™s a toy example showing how the cache works with small dimensions:</p><div class=\"code-block\"># Demo: Small cache with 1 batch, 2 heads, head_dim=4\ncache = PagedKVCache(batch_size=1, num_heads=2, head_dim=4, page_size=4, max_pages=2, device='cpu')\n\n# First update with 3 tokens\nk1 = torch.randn(1, 2, 3, 4)  # [batch, heads, tokens, head_dim]\nv1 = torch.randn(1, 2, 3, 4)\nk_out, v_out = cache.update(k1, v1)\nprint(\"After first update:\", k_out.shape)  # torch.Size([1, 2, 3, 4])\n\n# Second update with 5 tokens (spills into 2nd page)\nk2 = torch.randn(1, 2, 5, 4)\nv2 = torch.randn(1, 2, 5, 4)\nk_out, v_out = cache.update(k2, v2)\nprint(\"After second update:\", k_out.shape)  # torch.Size([1, 2, 8, 4])</div><p><strong>Output Explanation:</strong></p><ul><li>After the first update, the cache stores 3 tokens.</li><li>After the second update, total length is 8 tokens (3 old + 5 new). The system automatically uses multiple pages to hold them.</li></ul><p>This small demo makes it clear how the cache dynamically grows while avoiding large contiguous allocations, just like virtual memory paging in operating systems.</p>",
    "toc": [
        "Introduction to Paged KV Cache",
        "The Problem with Traditional KV Caches",
        "Paged Solution Architecture",
        "Beginner-Friendly Demo"
    ]
}
