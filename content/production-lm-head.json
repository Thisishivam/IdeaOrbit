{
  "title": "Production LM Head with Safe Weight Tying",
  "meta": {
    "date": "Sep 13, 2025",
    "readTime": "15 min read"
  },
  "tags": ["LM Head", "Weight Tying", "Memory Optimization", "Production"],
  "content": "<h2 id=\"introduction\">Introduction to Production LM Head</h2><p>The Language Modeling (LM) Head is the final projection layer in a transformer model. It maps hidden states into logits over the vocabulary. Our production implementation ensures safe weight tying, efficient initialization, and avoids pitfalls like device/dtype mismatches or duplicated memory usage.</p><h2 id=\"problem\">Common LM Head Issues</h2><ul><li><strong>Device Mismatches:</strong> Embedding and LM head stored on different devices</li><li><strong>Dtype Conflicts:</strong> Errors in mixed precision training</li><li><strong>Redundant Parameters:</strong> LM head and embedding duplicating weights</li><li><strong>Gradient Flow Errors:</strong> Improper tying can break updates</li></ul><h2 id=\"solution\">Production-Ready Implementation</h2><div class=\"code-block\">class ProductionLMHead(nn.Module):\n    \"\"\"Language modeling head with weight tying support\"\"\"\n    def __init__(self, hidden_dim: int, vocab_size: int, bias: bool = False):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        \n        # LM head weight (to be safely tied with embedding)\n        self.weight = nn.Parameter(torch.empty(vocab_size, hidden_dim))\n        \n        if bias:\n            self.bias = nn.Parameter(torch.zeros(vocab_size))\n        else:\n            self.bias = None\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute logits from hidden states\"\"\"\n        if self.bias is None:\n            return torch.matmul(x, self.weight.t())\n        else:\n            return F.linear(x, self.weight, self.bias)\n\n# In ProductionGPT.__init__\nself.lm_head = ProductionLMHead(\n    hidden_dim=config.hidden_dim,\n    vocab_size=config.vocab_size,\n    bias=config.use_bias\n)\n\n# Safe weight tying (embedding <-> LM head)\nif config.tie_weights:\n    self.lm_head.weight = self.token_embedding.weight\n    \n# GPT-4 style initialization\ndef _init_weights(self, module):\n    if isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2))\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2))\n\n# Example configuration\nconfig = GPTConfig(\n    vocab_size=100256,\n    hidden_dim=8192,\n    tie_weights=True,      # Enable weight tying\n    use_bias=False,        # Disable bias for efficiency\n)\n\nclass ProductionGPT(nn.Module):\n    def forward(self, input_ids: torch.Tensor, ...):\n        # ... transformer layers ...\n        \n        # Final normalization\n        x = self.final_norm(x)\n        \n        # LM head with safe weight tying\n        logits = self.lm_head(x)\n        return {'logits': logits, 'loss': loss}\n\n# Debugging checks\nassert model.lm_head.weight is model.token_embedding.weight\nassert model.lm_head.weight.device == model.token_embedding.weight.device\nassert model.lm_head.weight.shape == (config.vocab_size, config.hidden_dim)\n\n# Forward pass sanity check\nlogits = model.lm_head(hidden_states)\nassert logits.shape == (batch_size, seq_len, config.vocab_size)</div><h2 id=\"detailed-explanation\">Detailed Explanation of the Code</h2><ol><li><strong>LM Head Class:</strong> Defines a projection from hidden dimension → vocabulary logits. It creates a <code>weight</code> parameter (vocab_size × hidden_dim) and optional <code>bias</code>.</li><li><strong>Forward Pass:</strong> Uses either <code>torch.matmul</code> (no bias) or <code>F.linear</code> (with bias) for efficiency.</li><li><strong>Integration with GPT:</strong> The LM head is instantiated inside <code>ProductionGPT</code> and called after the transformer layers + normalization.</li><li><strong>Weight Tying:</strong> <code>self.lm_head.weight = self.token_embedding.weight</code> ensures embeddings and LM head share the same parameter. This halves memory usage and improves training consistency.</li><li><strong>Initialization:</strong> Follows GPT-4 style (scaled normal init) for stability and reproducibility.</li><li><strong>Validation Checks:</strong> Assert statements confirm correct shapes, devices, and tying.</li></ol><h2 id=\"demo\">Beginner-Friendly Demo</h2><p>Here’s a minimal runnable example showing weight tying in action:</p><div class=\"code-block\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Tiny config\nvocab_size = 100\nhidden_dim = 16\nseq_len = 5\nbatch_size = 2\n\n# Define token embedding\nembedding = nn.Embedding(vocab_size, hidden_dim)\n\n# Define LM head and tie weights\nlm_head = ProductionLMHead(hidden_dim, vocab_size)\nlm_head.weight = embedding.weight  # safe tying\n\n# Input: random token IDs\ninput_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\nhidden_states = embedding(input_ids)  # shape: [B, S, H]\n\n# Forward through LM head\nlogits = lm_head(hidden_states)\nprint(\"Logits shape:\", logits.shape)  # (B, S, vocab_size)</div><p><strong>What happens here?</strong></p><ul><li>The same weight matrix is used both for embedding lookup and for projecting hidden states to logits.</li><li>This reduces parameter count and ensures embeddings and output distribution are consistent.</li><li>Beginners can test by comparing <code>embedding.weight</code> and <code>lm_head.weight</code> — they point to the same tensor.</li></ul><h2 id=\"demo-memory\">Demo: Memory Usage Comparison</h2><p>To make this even clearer, here’s a small comparison of memory usage with vs without weight tying for the toy example above:</p><table class=\"comparison-table\">\n<tr><th>Setup</th><th>Embedding Params</th><th>LM Head Params</th><th>Total Params</th></tr>\n<tr><td>Without Weight Tying</td><td>100 × 16 = 1,600</td><td>100 × 16 = 1,600</td><td>3,200</td></tr>\n<tr><td>With Weight Tying</td><td>100 × 16 = 1,600</td><td>Shared with embedding</td><td>1,600</td></tr>\n</table><p><strong>Result:</strong> Weight tying cuts the parameter count in half. In large models (billions of parameters), this translates into gigabytes of VRAM savings.</p>",
  "toc": [
    "Introduction to Production LM Head",
    "Common LM Head Issues",
    "Production-Ready Implementation",
    "Detailed Explanation of the Code",
    "Beginner-Friendly Demo",
    "Demo: Memory Usage Comparison"
  ]
}
