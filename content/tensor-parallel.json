{
    "title": "Tensor-Parallel Linear Layers (Megatron Style)",
    "meta": {
        "date": "Sep 10, 2025",
        "readTime": "16 min read"
    },
    "tags": ["Tensor Parallelism", "Megatron", "Multi-GPU"],
    "toc": [
        "Concept",
        "ColumnParallelLinear",
        "RowParallelLinear",
        "GPT-4 Style Implementation",
        "Benefits",
        "Usage Pattern"
    ],
    "content": "<h2 id=\"concept\">Concept</h2><p>Distribute computation by splitting weights across devices, enabling training of models larger than what fits on a single GPU.</p><h2 id=\"column-parallel\">ColumnParallelLinear</h2><div class=\"code-block\">class ColumnParallelLinear(nn.Module):<br>&nbsp;&nbsp;def __init__(self, input_dim: int, output_dim: int, bias: bool = True):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;self.input_dim = input_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim = output_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.world_size = dist.get_world_size()<br>&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim_per_rank = output_dim // self.world_size<br><br>&nbsp;&nbsp;&nbsp;&nbsp;self.weight = nn.Parameter(torch.empty(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim_per_rank, self.input_dim,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;))<br><br>&nbsp;&nbsp;&nbsp;&nbsp;if bias:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = nn.Parameter(torch.zeros(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim_per_rank, device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.register_parameter('bias', None)<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor) -> torch.Tensor:<br>&nbsp;&nbsp;&nbsp;&nbsp;x = torch.matmul(x, self.weight.t())<br>&nbsp;&nbsp;&nbsp;&nbsp;if self.bias is not None:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = x + self.bias<br>&nbsp;&nbsp;&nbsp;&nbsp;return x</div><h2 id=\"row-parallel\">RowParallelLinear</h2><div class=\"code-block\">class RowParallelLinear(nn.Module):<br>&nbsp;&nbsp;def __init__(self, input_dim: int, output_dim: int, bias: bool = True):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;self.input_dim = input_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim = output_dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.world_size = dist.get_world_size()<br>&nbsp;&nbsp;&nbsp;&nbsp;self.input_dim_per_rank = input_dim // self.world_size<br><br>&nbsp;&nbsp;&nbsp;&nbsp;self.weight = nn.Parameter(torch.empty(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim, self.input_dim_per_rank,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;))<br><br>&nbsp;&nbsp;&nbsp;&nbsp;if bias:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = nn.Parameter(torch.zeros(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output_dim, device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.register_parameter('bias', None)<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor) -> torch.Tensor:<br>&nbsp;&nbsp;&nbsp;&nbsp;# Gather results from all devices<br>&nbsp;&nbsp;&nbsp;&nbsp;x = torch.matmul(x, self.weight.t())<br>&nbsp;&nbsp;&nbsp;&nbsp;x = all_gather(x)<br>&nbsp;&nbsp;&nbsp;&nbsp;if self.bias is not None:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = x + self.bias<br>&nbsp;&nbsp;&nbsp;&nbsp;return x</div><h2 id=\"gpt-4-style-implementation\">GPT-4 Style Implementation</h2><div class=\"code-block\">class ColumnParallelLinear(nn.Module):<br>&nbsp;&nbsp;\"\"\"Linear layer with column parallelism\"\"\"<br>&nbsp;&nbsp;def __init__(self, in_features: int, out_features: int, bias: bool = True):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;world_size = dist.get_world_size() if dist.is_initialized() else 1<br>&nbsp;&nbsp;&nbsp;&nbsp;self.out_features = out_features<br>&nbsp;&nbsp;&nbsp;&nbsp;self.out_features_per_rank = out_features // world_size<br><br>&nbsp;&nbsp;&nbsp;&nbsp;self.weight = nn.Parameter(torch.empty(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.out_features_per_rank, in_features,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;))<br><br>&nbsp;&nbsp;&nbsp;&nbsp;if bias:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = nn.Parameter(torch.zeros(self.out_features_per_rank, device='cuda', dtype=torch.bfloat16))<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = None<br><br>&nbsp;&nbsp;&nbsp;&nbsp;# GPT-4 style initialization<br>&nbsp;&nbsp;&nbsp;&nbsp;nn.init.normal_(self.weight, mean=0.0, std=0.02 / math.sqrt(2))<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor) -> torch.Tensor:<br>&nbsp;&nbsp;&nbsp;&nbsp;output = F.linear(x, self.weight, self.bias)<br>&nbsp;&nbsp;&nbsp;&nbsp;if dist.is_initialized() and dist.get_world_size() > 1:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dist.all_reduce(output, op=dist.ReduceOp.SUM)<br>&nbsp;&nbsp;&nbsp;&nbsp;return output</div><div class=\"code-block\">class RowParallelLinear(nn.Module):<br>&nbsp;&nbsp;\"\"\"Linear layer with row parallelism\"\"\"<br>&nbsp;&nbsp;def __init__(self, in_features: int, out_features: int, bias: bool = True):<br>&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;world_size = dist.get_world_size() if dist.is_initialized() else 1<br>&nbsp;&nbsp;&nbsp;&nbsp;self.in_features = in_features<br>&nbsp;&nbsp;&nbsp;&nbsp;self.in_features_per_rank = in_features // world_size<br><br>&nbsp;&nbsp;&nbsp;&nbsp;self.weight = nn.Parameter(torch.empty(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;out_features, self.in_features_per_rank,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device='cuda', dtype=torch.bfloat16<br>&nbsp;&nbsp;&nbsp;&nbsp;))<br><br>&nbsp;&nbsp;&nbsp;&nbsp;if bias:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = nn.Parameter(torch.zeros(out_features, device='cuda', dtype=torch.bfloat16))<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias = None<br><br>&nbsp;&nbsp;&nbsp;&nbsp;nn.init.normal_(self.weight, mean=0.0, std=0.02 / math.sqrt(2))<br><br>&nbsp;&nbsp;def forward(self, x: torch.Tensor) -> torch.Tensor:<br>&nbsp;&nbsp;&nbsp;&nbsp;# Split input along last dimension<br>&nbsp;&nbsp;&nbsp;&nbsp;if dist.is_initialized() and dist.get_world_size() > 1:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_split = x.chunk(dist.get_world_size(), dim=-1)[dist.get_rank()]<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_split = x<br><br>&nbsp;&nbsp;&nbsp;&nbsp;output = F.linear(x_split, self.weight, self.bias)<br>&nbsp;&nbsp;&nbsp;&nbsp;return output</div><h4>Detailed Explanation of the GPT-4 Style Implementation</h4><ul><li><strong>World Size Handling:</strong> The code safely checks if distributed training is initialized, defaulting to single-device operation if not, making it robust across environments.</li><li><strong>Weight Partitioning:</strong> The weights are split either by columns (output features) or by rows (input features), reducing memory load per GPU while keeping computation balanced.</li><li><strong>Data Type Optimization:</strong> Storing weights in <code>torch.bfloat16</code> saves memory and speeds up computation without compromising precision, especially for transformer-scale models.</li><li><strong>Initialization:</strong> Using a scaled normal distribution ensures stable training dynamics, as seen in GPT-4 and other large models.</li><li><strong>Bias Handling:</strong> Bias terms are conditionally added, and set to zero where appropriate, minimizing unnecessary computation.</li><li><strong>Synchronization:</strong> The <code>all_reduce</code> operation in <code>ColumnParallelLinear</code> ensures that outputs from all devices are summed, maintaining correctness in distributed settings.</li><li><strong>Input Splitting:</strong> The <code>RowParallelLinear</code> layer slices input across devices based on rank, ensuring parallel computation while avoiding unnecessary communication overhead.</li></ul><p>These implementations follow GPT-4 best practices, balancing scalability, speed, and memory efficiency while making it easier for users to experiment with large-scale models across multiple GPUs.</p><h2 id=\"benefits\">Benefits</h2><ul><li>Scales to large models by reducing memory footprint per device</li><li>Enables training of models with billions of parameters</li><li>Maintains high GPU utilization across multiple devices</li><li>Seamless integration with DDP for combined data and tensor parallelism</li></ul><h2 id=\"usage\">Usage Pattern</h2><p>ColumnParallelLinear is typically used for the first linear layer in a block, while RowParallelLinear is used for the second linear layer, creating a balanced distribution of computation and memory usage.</p>"
}
