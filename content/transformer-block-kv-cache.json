{
  "title": "Transformer Block with Real KV Cache & Checkpointing",
  "meta": {
    "date": "Sep 13, 2025",
    "readTime": "20 min read"
  },
  "tags": ["KV Cache", "Gradient Checkpointing", "Transformer", "Production"],
  "toc": [
    "Enhanced Transformer Block Architecture",
    "Full Production Code",
    "Step-by-Step Explanation",
    "Beginner-Friendly Demo"
  ],
  "content": "<h2 id=\"introduction\">Enhanced Transformer Block Architecture</h2><p>This production-grade transformer block implementation combines real gradient checkpointing with efficient KV cache support. It allows training of large models without running out of memory, while also providing fast inference with cached key-value states.</p><h2 id=\"full-code\">Full Production Code</h2><div class=\"code-block\">class TransformerBlockWithKVCache(nn.Module):\n    def __init__(self,\n                 hidden_dim: int,\n                 num_heads: int,\n                 rotary_dim: int,\n                 layer_id: int,\n                 num_layers: int,\n                 dropout: float = 0.0,\n                 mlp_ratio: int = 4,\n                 use_checkpointing: bool = True):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        self.layer_id = layer_id\n        self.num_layers = num_layers\n        self.use_checkpointing = use_checkpointing\n        self.dropout = dropout\n\n        # Attention components\n        self.attn_norm = RMSNorm(hidden_dim)\n        self.qkv_proj = ProductionQKVProjection(hidden_dim, num_heads, rotary_dim)\n        self.attention = FlashAttentionV2(hidden_dim // num_heads, dropout=self.dropout, causal=True)\n        self.attn_out = RowParallelLinear(hidden_dim, hidden_dim, bias=False)\n        \n        # MLP components with SwiGLU\n        self.mlp_norm = RMSNorm(hidden_dim)\n        self.mlp = SwiGLU(hidden_dim, mlp_ratio)\n\n        # GPT-4 style initialization with layer scaling\n        self._init_weights()\n\n    def forward(self, x: torch.Tensor, positions: torch.Tensor, \n                use_cache: bool = False, cache: Optional[Any] = None):\n        \"\"\"Forward pass with KV cache support and gradient checkpointing\"\"\"\n        if self.use_checkpointing and self.training and not use_cache:\n            return torch.utils.checkpoint.checkpoint(\n                self._forward_impl, \n                x, positions, use_cache, cache,\n                use_reentrant=False,\n                preserve_rng_state=True\n            )\n        else:\n            return self._forward_impl(x, positions, use_cache, cache)\n\n    def _forward_impl(self, x: torch.Tensor, positions: torch.Tensor,\n                     use_cache: bool = False, cache: Optional[Any] = None):\n        \"\"\"Actual forward implementation\"\"\"\n        # Attention sub-block\n        residual = x\n        x_norm = self.attn_norm(x)\n        q, k, v = self.qkv_proj(x_norm, positions)\n        \n        if use_cache and cache is not None:\n            k, v = cache.update(k, v)\n        \n        attn_output = self.attention(q, k, v)\n\n        batch_size, seq_len = x.shape[0], x.shape[1]\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n        attn_output = self.attn_out(attn_output)\n        return attn_output\n\n    def _init_weights(self):\n        attn_std = 0.02 / math.sqrt(2 * self.num_layers)\n        nn.init.normal_(self.attn_out.weight, mean=0.0, std=attn_std)\n\n# Config\nconfig = GPTConfig(\n    hidden_dim=8192,\n    num_layers=48,\n    num_heads=64,\n    use_checkpointing=True,\n    use_kv_cache=True,\n    use_paged_kv_cache=True,\n)\n\n# Training with checkpointing\nmodel.train()\nfor batch in dataloader:\n    outputs = model(input_ids=batch['input_ids'], labels=batch['labels'], use_cache=False)\n    loss = outputs['loss']\n    loss.backward()\n    optimizer.step()\n\n# Generation with KV cache\nmodel.eval()\nwith torch.no_grad():\n    generated = model.generate(\n        input_ids=prompt,\n        max_length=1000,\n        use_cache=True,\n        temperature=0.8,\n        top_p=0.9\n    )\n\n# Debugging checks\nmodel.train()\nassert model.layers[0].use_checkpointing == True\n\nmodel.eval()\nassert hasattr(model, '_create_kv_cache')\n\nwith torch.no_grad():\n    outputs_train = model(input_ids, labels=labels)\n    outputs_infer = model(input_ids, use_cache=True)\n    torch.testing.assert_close(outputs_train['logits'], outputs_infer['logits'])</div>\n\n<h2 id=\"code-explanation\">Step-by-Step Explanation</h2>\n<ol>\n<li><strong>Initialization:</strong> The constructor sets up normalization layers, attention projection (QKV), FlashAttention, and MLP with SwiGLU. This mirrors GPT-4 style blocks.</li>\n<li><strong>forward:</strong>\n<ul>\n<li>If training + checkpointing enabled → wrap with <code>torch.utils.checkpoint</code>. This recomputes intermediate activations instead of storing them, saving memory.</li>\n<li>If inference or generation → call <code>_forward_impl</code> directly for speed.</li>\n</ul></li>\n<li><strong>_forward_impl:</strong>\n<ul>\n<li>Normalize hidden states, compute Q, K, V projections.</li>\n<li>If <code>use_cache=True</code>, the cache object updates with new K/V and returns the full sequence.</li>\n<li>Run attention → reshape heads → project back to hidden_dim.</li>\n</ul></li>\n<li><strong>_init_weights:</strong> Uses layer-scaled initialization (variance shrinks with depth), improving training stability in deep networks.</li>\n<li><strong>Config + Workflows:</strong>\n<ul>\n<li>During training → <code>use_checkpointing=True</code> reduces memory usage.</li>\n<li>During generation → <code>use_cache=True</code> reuses past keys/values for efficiency.</li>\n<li>Validation checks ensure checkpointing + cache work as expected.</li>\n</ul></li>\n</ol>\n\n<h2 id=\"beginner-demo\">Beginner-Friendly Demo</h2>\n<p>Here’s a tiny runnable example showing checkpointing and cache on a toy transformer block:</p>\n<div class=\"code-block\">import torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\n\nclass TinyBlock(nn.Module):\n    def __init__(self, hidden_dim=16):\n        super().__init__()\n        self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.use_checkpointing = True\n\n    def forward_impl(self, x):\n        return self.linear2(torch.relu(self.linear1(x)))\n\n    def forward(self, x):\n        if self.use_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self.forward_impl, x)\n        return self.forward_impl(x)\n\n# Fake KV cache: store previous outputs\nclass SimpleCache:\n    def __init__(self):\n        self.values = []\n    def update(self, v):\n        self.values.append(v)\n        return torch.cat(self.values, dim=1)\n\n# Demo\nblock = TinyBlock()\ncache = SimpleCache()\n\nx1 = torch.randn(1, 1, 16, requires_grad=True)\nx2 = torch.randn(1, 1, 16, requires_grad=True)\n\n# Training with checkpointing\nblock.train()\ny1 = block(x1)\nloss = y1.sum()\nloss.backward()\n\n# Generation with KV cache\nblock.eval()\nwith torch.no_grad():\n    out1 = block(x1)\n    full_seq = cache.update(out1)\n    out2 = block(x2)\n    full_seq = cache.update(out2)\n    print(\"Cached sequence shape:\", full_seq.shape)</div>\n\n<p><strong>What’s happening?</strong></p>\n<ul>\n<li><strong>Checkpointing:</strong> During training, activations from <code>linear1</code> aren’t stored. Instead, they are recomputed during backward pass, reducing memory usage.</li>\n<li><strong>Cache:</strong> During generation, outputs are appended to the cache. On the next step, the model doesn’t recompute the entire sequence—only the new token is added.</li>\n</ul>\n<p>This tiny demo illustrates the same principles applied in the full production block.</p>\n\n<p><em>Would you like me to also add a memory usage comparison table for the toy demo (like “checkpointing ON vs OFF, cache ON vs OFF”), similar to what we did for the LM Head article? That could help beginners visualize the savings.</em></p>"
}
